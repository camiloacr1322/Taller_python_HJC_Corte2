# -*- coding: utf-8 -*-
"""Copia de prueba python

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yAvk-VFNFdbhWbSnVN7AvwvdcQZKqL-
"""

!pip install scikit-posthocs

!pip install prince



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
from scipy.stats import shapiro
from scipy.stats import levene
from scipy.stats import f_oneway
import seaborn as sns
from scipy.stats import kruskal
import scikit_posthocs as sp
import scipy.stats as stats
import seaborn as sns
from scipy.stats import chi2_contingency
import prince
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


df = pd.read_excel('/content/base_datos_completa_NNA_TI_anon.xlsx', sheet_name='BD')



# Lista de columnas que quieres conservar
cols_demograficas = [
    "Id_fic","NACIONALIDAD", "SEXO", "ESTADO CIVIL",
    "EDAD", "CURSO DE VIDA", "ETNIA",
    "POBLACIÓN DIFERENCIAL Y DE INCLUSIÓN", "OCUPACIÓN",
    "IdNivelEducativo","VÍNCULO CON EL JEFE DE HOGAR",
    "Localidad_fic","ESTRATO SOCIOECONÓMICO"
]

# Crear un nuevo DataFrame solo con esas variables
af = df[cols_demograficas].copy()

"""Se trabajan con solo estas variables"""

af = af.replace(99999, pd.NA)

"""Codigo de duplicados"""

# Verificar si hay IDs duplicados
duplicados = df['Id_fic'].duplicated()
hay_duplicados = duplicados.any()

print("Hay IDs duplicados:", hay_duplicados)

# Ver cuáles IDs están duplicados
print("IDs duplicados:")
print(df.loc[duplicados, 'Id_fic'])

num_unicos = df['Id_fic'].nunique()
num_total = len(df)

print(f"IDs únicos: {num_unicos}, Total filas: {num_total}")
print("Todos los IDs son únicos?", num_unicos == num_total)
print("Numero de ID duplicados", num_total-num_unicos)

af_sin_duplicados = af.drop_duplicates(subset='Id_fic', keep='first')
af_sin_duplicados.shape

"""Codigo de faltantes"""

faltantes = af_sin_duplicados.isna().sum().sort_values(ascending=False)

# Mostrar siempre todas las columnas
print("Valores faltantes por variable:\n")
print(faltantes)

# --- Gráfico ---
plt.figure(figsize=(12, 6))
faltantes.plot(kind='bar', color='skyblue', edgecolor='black')

plt.title("Valores faltantes por variable", fontsize=14, weight='bold')
plt.xlabel("Variable")
plt.ylabel("Número de valores faltantes")
plt.xticks(rotation=90)
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Elimina filas con al menos un valor faltante
af_completa = af_sin_duplicados.dropna()

# Mostrar cuántas filas quedaron
print(f"Filas originales: {af_sin_duplicados.shape[0]}")
print(f"Filas después de eliminar faltantes: {af_completa.shape[0]}")

# Opcional: verificación rápida de que ya no hay NA
print("\n¿Aún hay valores faltantes?")
print(af_completa.isna().sum().sum() == 0)

af_completa.to_excel("af_completa.xlsx", index=False)

# Mantener solo las filas donde 'ESTRATO SOCIOECONÓMICO' no tiene valores faltantes
af_filtrado = af_sin_duplicados.dropna(subset=["ESTRATO SOCIOECONÓMICO"])

# Mostrar información del resultado
print(f"Filas originales: {af_sin_duplicados.shape[0]}")
print(f"Filas después de eliminar faltantes en 'ESTRATO SOCIOECONÓMICO': {af_filtrado.shape[0]}")

# Comprobar si aún hay nulos en esa variable
print("\nValores faltantes restantes en 'ESTRATO SOCIOECONÓMICO':")
print(af_filtrado['ESTRATO SOCIOECONÓMICO'].isna().sum())

af_filtrado.to_excel("af_filtrado.xlsx", index=False)

tabla_univar = af_filtrado["ESTRATO SOCIOECONÓMICO"].value_counts().reset_index()
tabla_univar.columns = ["ESTRATO SOCIOECONÓMICO", "Frecuencia"]
print(tabla_univar)

plt.figure(figsize=(10, 6))
plt.bar(tabla_univar["ESTRATO SOCIOECONÓMICO"], tabla_univar["Frecuencia"], color='skyblue')

# Añadir etiquetas de frecuencia
for i, frecuencia in enumerate(tabla_univar["Frecuencia"]):
    plt.text(i, frecuencia + 100, str(frecuencia), ha='center', va='bottom')


plt.title("Distribución por Estrato Socioeconómico")
plt.xlabel("Estrato Socioeconómico")
plt.ylabel("Frecuencia")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Con totales
pd.crosstab(af_filtrado["CURSO DE VIDA"], af_filtrado["ESTRATO SOCIOECONÓMICO"], margins=True, dropna=False)

# Porcentajes por columna
#pd.crosstab(af["SEXO"], af["ESTRATO SOCIOECONÓMICO"], normalize="columns") * 100

# Porcentajes por fila
#pd.crosstab(af["SEXO"], af["ESTRATO SOCIOECONÓMICO"], normalize="index") * 100

# Crear una tabla de contingencia para la frecuencia de CURSO DE VIDA y ESTRATO SOCIOECONÓMICO
crosstab_curso_estrato = pd.crosstab(af["CURSO DE VIDA"], af["ESTRATO SOCIOECONÓMICO"])

# Crear el mapa de calor
plt.figure(figsize=(12, 8))
sns.heatmap(crosstab_curso_estrato, annot=True, fmt='d', cmap='YlGnBu', linewidths=.5)

plt.title("Distribución de Curso de Vida por Estrato Socioeconómico (Mapa de Calor)")
plt.xlabel("Estrato Socioeconómico")
plt.ylabel("Curso de Vida")
plt.tight_layout()
plt.show()

# Create a cross-tabulation for educational level by socioeconomic stratum, including combinations with no data
crosstab_nivel_estrato = pd.crosstab(af["IdNivelEducativo"], af["ESTRATO SOCIOECONÓMICO"], dropna=False)

# Create the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(crosstab_nivel_estrato, annot=True, fmt='d', cmap='YlGnBu', linewidths=.5)

plt.title("Distribución del Nivel Educativo por Estrato Socioeconómico (Mapa de Calor)")
plt.xlabel("Estrato Socioeconómico")
plt.ylabel("Nivel Educativo")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Asegurar tipo numérico de edad
af_filtrado["EDAD"] = pd.to_numeric(af_filtrado["EDAD"], errors="coerce")

# Agrupar por estrato socioeconómico y calcular media y tamaño de muestra
tabla_media = (
    af_filtrado.groupby("ESTRATO SOCIOECONÓMICO", dropna=False)["EDAD"]
    .agg(Media_Edad="mean", N="count")
    .reset_index()
)

# Mostrar tabla resultante
print("📊 Media de edad y número de casos por estrato socioeconómico:\n")
display(tabla_media)

# Crear la gráfica de barras
plt.figure(figsize=(12, 8)) # Aumentar tamaño de la figura
bars = plt.bar(tabla_media["ESTRATO SOCIOECONÓMICO"], tabla_media["Media_Edad"], color='skyblue', edgecolor='black') # Cambiar color y añadir borde

# Añadir etiquetas de frecuencia (N) y media de edad en las barras
for bar, n, mean_edad in zip(bars, tabla_media["N"], tabla_media["Media_Edad"]):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
             f'N={n}\nMedia={mean_edad:.2f}',
             ha='center', va='bottom', fontsize=9)

# Añadir un poco de espacio extra en la parte superior del gráfico para las etiquetas
plt.ylim(0, tabla_media["Media_Edad"].max() * 1.2)


# Añadir etiquetas
plt.title("Media de Edad por Estrato Socioeconómico con Frecuencia y Media", fontsize=16, weight='bold') # Aumentar tamaño y poner negrita
plt.xlabel("Estrato Socioeconómico", fontsize=12) # Aumentar tamaño
plt.ylabel("Media de Edad", fontsize=12) # Aumentar tamaño
plt.xticks(rotation=45, ha='right', fontsize=10) # Aumentar tamaño y ajustar rotación
plt.yticks(fontsize=10) # Aumentar tamaño
plt.grid(axis='y', linestyle='--', alpha=0.7) # Añadir cuadrícula
plt.tight_layout()
plt.show()

# Asegurar tipo numérico de edad y eliminar filas con NaN en ESTRATO SOCIOECONÓMICO
af_filtered_for_density = af.dropna(subset=["EDAD", "ESTRATO SOCIOECONÓMICO"]).copy()
af_filtered_for_density["EDAD"] = pd.to_numeric(af_filtered_for_density["EDAD"], errors="coerce")


# Create a FacetGrid for density plots of Age by Socioeconomic Stratum
g = sns.FacetGrid(af_filtered_for_density, col="ESTRATO SOCIOECONÓMICO", col_wrap=3, height=4, sharey=False)
g.map(sns.kdeplot, "EDAD", fill=True)

g.fig.suptitle("Distribución de Densidad de Edad por Estrato Socioeconómico (Gráficos Separados)", y=1.02)
g.set_titles("Estrato: {col_name}")
g.set_axis_labels("Edad", "Densidad")
plt.tight_layout()
plt.show()

for estrato, grupo in af_filtrado.groupby("ESTRATO SOCIOECONÓMICO"):
    stat, p = shapiro(grupo["EDAD"].dropna())
    print(f"Estrato {estrato}: p-valor = {p:.4f}")

"""Shapiro no funciona por el N de los grupos"""

grupos = [g["EDAD"].dropna() for _, g in af_filtrado.groupby("ESTRATO SOCIOECONÓMICO")]
stat, p = levene(*grupos)
print(f"Prueba de Levene: p-valor = {p:.4f}")

"""Leven no se afecta por nada y no se recahza hipotesis nula, las varinzas son homecdasticas, supuesto cumplido"""

grupos = [g["EDAD"].dropna() for _, g in af_filtrado.groupby("ESTRATO SOCIOECONÓMICO")]
stat, p = f_oneway(*grupos)
print(f"ANOVA: F = {stat:.3f}, p-valor = {p:.4f}")

"""La pruea Anova nos da diferencias en los grupo, pero no es significativo"""

plt.figure(figsize=(8, 5))
sns.boxplot(x="ESTRATO SOCIOECONÓMICO", y="EDAD", data=af_filtrado)
plt.title("Boxplot de la Edad por Estrato Socioeconómico")
plt.show()

# Filtrar solo estratos con al menos 3 observaciones
conteo = af_filtrado.groupby("ESTRATO SOCIOECONÓMICO")["EDAD"].count()
estratos_validos = conteo[conteo >= 3].index
af_validos = af_filtrado[af_filtrado["ESTRATO SOCIOECONÓMICO"].isin(estratos_validos)]

# Ejecutar prueba de Kruskal
grupos = [g["EDAD"].dropna() for _, g in af_validos.groupby("ESTRATO SOCIOECONÓMICO")]
stat, p = kruskal(*grupos)

print(f"📊 Prueba de Kruskal-Wallis: H = {stat:.3f}, p-valor = {p:.4f}")

if p < 0.05:
    print("➡️ Hay diferencias significativas en la edad entre los estratos.")
else:
    print("➡️ No se detectan diferencias significativas entre los estratos.")

# --- 1️⃣ Filtrar el estrato 6 ---
af_sin6 = af_filtrado[af_filtrado["ESTRATO SOCIOECONÓMICO"] != "6. Alto"]

# --- 2️⃣ Prueba de Kruskal–Wallis ---
grupos = [grupo["EDAD"].dropna() for _, grupo in af_sin6.groupby("ESTRATO SOCIOECONÓMICO")]
kw_stat, kw_p = stats.kruskal(*grupos)

print(f"📊 Resultado de Kruskal–Wallis (sin estrato 6): estadístico={kw_stat:.4f}, p-valor={kw_p:.4f}")

# --- 3️⃣ Prueba post hoc de Dunn ---
posthoc = sp.posthoc_dunn(
    af_sin6,
    val_col="EDAD",
    group_col="ESTRATO SOCIOECONÓMICO",
    p_adjust="bonferroni"
)

print("\n🔎 Comparaciones múltiples (Dunn con corrección Bonferroni):")
display(posthoc)

"""Se observa que las medias de la edad de los estratos son entre 3 y 1"""

# Tabla de conteo entre Localidad y Estrato
tabla_conteo = pd.crosstab(
    af_filtrado["Localidad_fic"],
    af_filtrado["ESTRATO SOCIOECONÓMICO"],
    dropna=False
)

print("📊 Conteo de registros por Localidad y Estrato:\n")
display(tabla_conteo)

# Tabla de proporciones por fila (dentro de cada localidad)
tabla_prop = pd.crosstab(
    af_filtrado["Localidad_fic"],
    af_filtrado["ESTRATO SOCIOECONÓMICO"],
    normalize="index"
) * 100

print("📈 Porcentaje por estrato dentro de cada localidad:\n")
display(tabla_prop.round(2))

chi2, p, dof, expected = chi2_contingency(tabla_conteo)

print(f"📊 Chi-cuadrado = {chi2:.4f}, gl = {dof}, p-valor = {p:.4f}")
if p < 0.05:
    print("✅ Existe asociación significativa entre localidad y estrato.")
else:
    print("❌ No se encontró asociación significativa entre localidad y estrato.")

"""Analisis de correspondencia"""

tabla = pd.crosstab(af_filtrado["Localidad_fic"], af_filtrado["ESTRATO SOCIOECONÓMICO"], dropna=False)
print("Tabla de contingencia (muestra):")
display(tabla.head())
# Crear modelo de correspondencias simples
ca = prince.CA(n_components=5, n_iter=10, copy=True, check_input=True, engine='sklearn', random_state=42)
ca = ca.fit(tabla)

# coordenadas principales (filas y columnas)
rows_coords = ca.row_coordinates(tabla)      # filas: localidades
cols_coords = ca.column_coordinates(tabla)   # columnas: estratos

# autovalores (inercia por dimensión) y % explicado
# prince expone 'eigenvalues_' y 'explained_inertia_' en versiones recientes; si no, se usan eigenvalues_
try:
    eigen = np.array(ca.eigenvalues_)
except Exception:
    eigen = np.array(ca.eigenvalues_)
explained = eigen / eigen.sum()
cum_explained = explained.cumsum()

print("\nAutovalores (inercia) por dimensión:", np.round(eigen, 6))
print("Porcentaje de varianza explicada por dimensión (%):", np.round(explained * 100, 3))
print("Varianza explicada acumulada (%):", np.round(cum_explained * 100, 3))

# 3.4 Cálculo práctico de cos² (por fila y por columna) sobre las 2 dimensiones
# cos2_ik = (coord_ik^2) / (sum over dims coord_ij^2)
rows_sq = (rows_coords**2)
rows_dist2 = rows_sq.sum(axis=1)
rows_cos2 = rows_sq.divide(rows_dist2, axis=0).fillna(0)   # proporción de la inercia por dimensión

cols_sq = (cols_coords**2)
cols_dist2 = cols_sq.sum(axis=1)
cols_cos2 = cols_sq.divide(cols_dist2, axis=0).fillna(0)

# 3.5 Contribuciones aproximadas por dimensión
# Fórmula práctica: cont_ik = (coord_ik^2) / eigen_k, luego normalizar a % por dimensión.
cont_rows = pd.DataFrame(index=rows_coords.index)
cont_cols = pd.DataFrame(index=cols_coords.index)
for k in range(rows_coords.shape[1]):
    cont_rows[f"Dim{k+1}"] = (rows_coords.iloc[:, k]**2) / eigen[k]
    cont_cols[f"Dim{k+1}"] = (cols_coords.iloc[:, k]**2) / eigen[k]

# Normalizar para que sumen 1 (por dimensión)
cont_rows_pct = cont_rows.divide(cont_rows.sum(axis=0), axis=1) * 100
cont_cols_pct = cont_cols.divide(cont_cols.sum(axis=0), axis=1) * 100

# Mostrar tablas resumidas
print("\n--- Top contribuciones filas (Localidades) por Dim1 ---")
display(cont_rows_pct["Dim1"].sort_values(ascending=False).head(10))

print("\n--- Top contribuciones columnas (Estratos) por Dim1 ---")
display(cont_cols_pct["Dim1"].sort_values(ascending=False).head(10))

# Coordenadas de filas y columnas
row_coords = ca.row_coordinates(tabla)
col_coords = ca.column_coordinates(tabla)

plt.figure(figsize=(8,6))

# Graficar filas (categorías de CURSO DE VIDA)
plt.scatter(row_coords[0], row_coords[1], label='Filas (Localidades)')
for i, txt in enumerate(row_coords.index):
    plt.text(row_coords.iloc[i,0], row_coords.iloc[i,1], txt)

# Graficar columnas (categorías de ESTRATO SOCIOECONÓMICO)
plt.scatter(col_coords[0], col_coords[1], marker='s', label='Columnas (Estrato)', alpha=0.7)
for i, txt in enumerate(col_coords.index):
    plt.text(col_coords.iloc[i,0], col_coords.iloc[i,1], txt)

plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)

plt.title("Correspondencias Simples - Dimensiones 1 y 2")
plt.xlabel("Dimensión 1")
plt.ylabel("Dimensión 2")
plt.legend()
plt.show()

# Cluster

#ruta_xlsx = "RUTA DE LA BASE DE DATOS"
#df = pd.read_excel(ruta_xlsx)
vars_cat = ["SEXO", "CURSO DE VIDA", "ETNIA", "OCUPACIÓN", "Localidad_fic"]
X_cat = df[vars_cat].copy()
X_cat = X_cat.dropna().reset_index(drop=True)
print("Dimensiones X_cat:", X_cat.shape)
print(X_cat.head())

## One-Hot encoding para usar un método de clúster estándar

X_enc = pd.get_dummies(X_cat, drop_first=False)
print("Dimensiones codificadas:", X_enc.shape)

ks = range(2, 9)
inertias = []
sil_scores = []

for k in ks:
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(X_enc)
    inertias.append(km.inertia_)
    sil_scores.append(silhouette_score(X_enc, labels))

plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(ks, inertias, marker='o')
plt.title("Curva del codo (Inertia)")
plt.xlabel("k"); plt.ylabel("Inertia")

plt.subplot(1,2,2)
plt.plot(ks, sil_scores, marker='o')
plt.title("Silhouette vs k")
plt.xlabel("k"); plt.ylabel("Silhouette")
plt.tight_layout()
plt.show()

print("Inertias:", dict(zip(ks, inertias)))
print("Silhouette:", dict(zip(ks, sil_scores)))

k_final = 6  
km = KMeans(n_clusters=k_final, n_init=20, random_state=42)
cluster = km.fit_predict(X_enc)
df_clusters = X_cat.copy()
df_clusters["cluster"] = cluster

vars_perfil = ["SEXO", "CURSO DE VIDA", "OCUPACIÓN", "Localidad_fic"]  # agrega "PROGRAMA" si la tienes


#Función: tabla % por clúster de una variable categórica

def tabla_porcentual(dfc, var):
    # % por clúster (filas = clusters)
    ct = pd.crosstab(dfc["cluster"], dfc[var], normalize="index") * 100
    return ct.round(1)

# Mostrar top-k categorías por clúster para cada variable
def topk_por_cluster(dfc, var, k=5):
    ct = pd.crosstab(dfc["cluster"], dfc[var], normalize="index") * 100
    top = {}
    for cl in sorted(dfc["cluster"].unique()):
        fila = ct.loc[cl].sort_values(ascending=False).head(k)
        top[cl] = fila
    return top  

for v in vars_perfil:
    print(f"\n== {v} (% por clúster) ==")
    print(tabla_porcentual(df_clusters, v))
    top = topk_por_cluster(df_clusters, v, k=5)
    print(f"\nTop-5 categorías de {v} por clúster:")
    for cl, serie in top.items():
        print(f"  cluster {cl}:\n{serie.to_string()}\n")

# Heatmap por variable (porcentaje dentro de clúster)
for v in vars_perfil:
    plt.figure(figsize=(12, 4 + 0.2*len(df_clusters[v].unique())))
    ct = tabla_porcentual(df_clusters, v)
    sns.heatmap(ct, annot=True, fmt=".1f", cmap="Blues")
    plt.title(f"{v} - % por clúster")
    plt.xlabel(v)
    plt.ylabel("cluster")
    plt.tight_layout()
    plt.show()

# Barras apiladas por variable (porcentaje dentro de clúster)
for v in vars_perfil:
    ct = pd.crosstab(df_clusters["cluster"], df_clusters[v], normalize="index") * 100
    ax = ct.plot(kind="bar", stacked=True, figsize=(12,5))
    plt.ylabel("% dentro de clúster")
    plt.title(f"Distribución de {v} por clúster")
    plt.legend(bbox_to_anchor=(1.02, 1), loc="upper left", ncol=1)
    plt.tight_layout()
    plt.show()

# Correspondencia clúster × estrato 
if "ESTRATO SOCIOECONÓMICO" in df.columns:
    tmp = df.loc[df_clusters.index] 
    cruz = pd.crosstab(tmp["ESTRATO SOCIOECONÓMICO"], df_clusters["cluster"], normalize="index") * 100
    print("\n== ESTRATO SOCIOECONÓMICO (% filas por estrato, columnas = clúster) ==")
    print(cruz.round(1))
